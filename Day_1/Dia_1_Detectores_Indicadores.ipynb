{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e678a77",
   "metadata": {},
   "source": [
    "# Día 1 — Detectores de IA: Indicadores y Métricas (Colab)\n",
    "\n",
    "Este cuaderno te guía para **calcular indicadores** (perplejidad, burstiness, estilometría) en textos y **evaluar detectores** (TP/FP/FN, Precisión, Recall, F1) desde un CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1980c",
   "metadata": {},
   "source": [
    "## 0) Instalación de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0223f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers==4.44.2 torch --upgrade\n",
    "!pip -q install nltk==3.9.1 textstat==0.7.4 pandas==2.2.2 matplotlib==3.9.0 tqdm==4.66.5\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "try:\n",
    "    nltk.download('punkt_tab')\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5885c",
   "metadata": {},
   "source": [
    "## 1) Importaciones y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re, statistics\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import textstat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce87c42",
   "metadata": {},
   "source": [
    "## 2) Modelo ligero para **Perplejidad**\n",
    "\n",
    "Usaremos un modelo causal (GPT-2 pequeño) para estimar **perplejidad**:\n",
    "$\\mathrm{PPL} = e^{\\mathrm{cross\\text{-}entropy}}$.\n",
    "\n",
    "Valores **bajos** suelen indicar texto más \"predecible\" (común en IA); valores **altos** sugieren mayor entropía (frecuente en humanos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11677995",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(text: str, stride: int = 512) -> float:\n",
    "    \"\"\"Calcula perplejidad por ventana (stride) para textos largos.\"\"\"\n",
    "    enc = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = enc['input_ids']\n",
    "    nlls = []\n",
    "    max_length = model.config.n_positions\n",
    "    for i in range(0, input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, input_ids.size(1))\n",
    "        trg_len = end_loc - i\n",
    "        input_ids_slice = input_ids[:, begin_loc:end_loc]\n",
    "        target_ids = input_ids_slice.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        outputs = model(input_ids_slice, labels=target_ids)\n",
    "        neg_log_likelihood = outputs.loss * trg_len\n",
    "        nlls.append(neg_log_likelihood)\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc).item()\n",
    "    return float(ppl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6edfb",
   "metadata": {},
   "source": [
    "## 3) **Burstiness** (variabilidad entre oraciones)\n",
    "\n",
    "Medimos la **variación relativa** en longitud de oraciones:\n",
    "$\\mathrm{Burstiness} = \\frac{\\sigma(\\text{long.orac})}{\\mu(\\text{long.orac}) + 1e{-8}}$.\n",
    "\n",
    "Valores **mayores** ⇒ más irregularidad humana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_burstiness(text: str) -> float:\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) <= 1:\n",
    "        return 0.0\n",
    "    lengths = [len(word_tokenize(s)) for s in sentences]\n",
    "    return float(np.std(lengths, ddof=1) / (np.mean(lengths) + 1e-8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869a669",
   "metadata": {},
   "source": [
    "## 4) **Estilometría** básica\n",
    "\n",
    "Rasgos simples y rápidos:\n",
    "\n",
    "- **TTR (Type-Token Ratio)** = vocabulario único / palabras totales\n",
    "- **Longitud media de oración** (tokens por oración)\n",
    "- **Longitud media de palabra**\n",
    "- **Puntuación por 100 palabras**\n",
    "- **% Funcionales** (aprox. pronombres, preps., conj.) vía POS de NLTK\n",
    "- **Legibilidad** (Flesch Reading Ease; `textstat`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTIONAL_TAGS = set(['PRP','PRP$','IN','CC','DT','TO','MD','UH'])\n",
    "\n",
    "def stylometry_features(text: str) -> Dict[str, float]:\n",
    "    words = [w for w in word_tokenize(text) if re.search(r'\\w', w)]\n",
    "    sents = sent_tokenize(text) or ['']\n",
    "    if len(words) == 0:\n",
    "        return {\n",
    "            'ttr': 0.0,\n",
    "            'sent_len_avg': 0.0,\n",
    "            'word_len_avg': 0.0,\n",
    "            'punct_per_100w': 0.0,\n",
    "            'functional_pct': 0.0,\n",
    "            'flesch': 0.0\n",
    "        }\n",
    "    ttr = len(set(w.lower() for w in words)) / len(words)\n",
    "    sent_len_avg = np.mean([len(word_tokenize(s)) for s in sents])\n",
    "    word_len_avg = np.mean([len(w) for w in words])\n",
    "    punct_count = len([c for c in text if c in '.,;:!?—–-…()[]{}\"\\''])\n",
    "    punct_per_100w = (punct_count / len(words)) * 100\n",
    "    tags = [t for _, t in pos_tag(words)]\n",
    "    functional_pct = (sum(1 for t in tags if t in FUNCTIONAL_TAGS) / len(tags)) * 100\n",
    "    try:\n",
    "        flesch = textstat.flesch_reading_ease(text)\n",
    "    except Exception:\n",
    "        flesch = 0.0\n",
    "    return {\n",
    "        'ttr': float(ttr),\n",
    "        'sent_len_avg': float(sent_len_avg),\n",
    "        'word_len_avg': float(word_len_avg),\n",
    "        'punct_per_100w': float(punct_per_100w),\n",
    "        'functional_pct': float(functional_pct),\n",
    "        'flesch': float(flesch)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292049d4",
   "metadata": {},
   "source": [
    "## 5) Procesamiento por carpeta\n",
    "\n",
    "Estructura esperada:\n",
    "\n",
    "```\n",
    "/content/textos/\n",
    "├── human/\n",
    "│   ├── human_01.txt\n",
    "│   └── ...\n",
    "└── ai/\n",
    "    ├── ai_01.txt\n",
    "    └── ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path('/content/textos')  # cambia si lo necesitas\n",
    "rows = []\n",
    "\n",
    "def read_txt(path: Path) -> str:\n",
    "    try:\n",
    "        return path.read_text(encoding='utf-8', errors='ignore')\n",
    "    except Exception:\n",
    "        return path.read_text(encoding='latin-1', errors='ignore')\n",
    "\n",
    "if INPUT_DIR.exists():\n",
    "    for cls in ['human', 'ai']:\n",
    "        for p in sorted((INPUT_DIR/cls).glob('*.txt')):\n",
    "            txt = read_txt(p)\n",
    "            ppl = compute_perplexity(txt)\n",
    "            burst = compute_burstiness(txt)\n",
    "            sty = stylometry_features(txt)\n",
    "            rec = {\n",
    "                'id': p.stem,\n",
    "                'true_label': cls,\n",
    "                'perplexity': ppl,\n",
    "                'burstiness': burst,\n",
    "                **sty\n",
    "            }\n",
    "            rows.append(rec)\n",
    "\n",
    "df_feats = pd.DataFrame(rows)\n",
    "if len(df_feats):\n",
    "    display(df_feats.head())\n",
    "    df_feats.to_csv('/content/indicadores_textos.csv', index=False)\n",
    "    print('Guardado: /content/indicadores_textos.csv')\n",
    "else:\n",
    "    print('No se encontró la carpeta /content/textos. Crea esta estructura para procesar archivos.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d8088",
   "metadata": {},
   "source": [
    "## 6) Visualización rápida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_feats' in globals() and len(df_feats):\n",
    "    for col in ['perplexity','burstiness','ttr','sent_len_avg','word_len_avg','punct_per_100w','functional_pct','flesch']:\n",
    "        plt.figure()\n",
    "        df_feats.boxplot(column=col, by='true_label')\n",
    "        plt.title(f'{col} por clase')\n",
    "        plt.suptitle('')\n",
    "        plt.xlabel('Clase real')\n",
    "        plt.ylabel(col)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c8682",
   "metadata": {},
   "source": [
    "## 7) **Evaluación de detectores** desde CSV\n",
    "\n",
    "Formato esperado (`results.csv`):\n",
    "\n",
    "| id_texto | detector | score | etiqueta | true_label |\n",
    "|---|---|---:|---|---|\n",
    "\n",
    "- `etiqueta`: salida del detector (`AI`, `HUMAN`, o `MIXTO` si aplica).\n",
    "- `true_label`: etiqueta real (`ai` o `human`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41408fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl = pd.DataFrame({\n",
    "    'id_texto': ['ai_01','human_01'],\n",
    "    'detector': ['DetectorX','DetectorX'],\n",
    "    'score': [0.87, 0.12],\n",
    "    'etiqueta': ['AI', 'HUMAN'],\n",
    "    'true_label': ['ai', 'human']\n",
    "})\n",
    "tpl.to_csv('/content/results_template.csv', index=False)\n",
    "tpl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b9a03",
   "metadata": {},
   "source": [
    "### Funciones de métricas (TP/FP/FN, Precisión, Recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion(df: pd.DataFrame, positive_label='ai') -> Dict[str, int]:\n",
    "    y_true = df['true_label'].str.lower()\n",
    "    y_pred = df['etiqueta'].str.upper().map({'AI':'ai','HUMAN':'human'})\n",
    "    mask = y_pred.isin(['ai','human'])\n",
    "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    TP = int(((y_true==positive_label) & (y_pred==positive_label)).sum())\n",
    "    FP = int(((y_true!='ai') & (y_pred==positive_label)).sum())\n",
    "    FN = int(((y_true==positive_label) & (y_pred!='ai')).sum())\n",
    "    TN = int(((y_true!='ai') & (y_pred!='ai')).sum())\n",
    "    return {'TP':TP, 'FP':FP, 'FN':FN, 'TN':TN}\n",
    "\n",
    "def prf_from_counts(c):\n",
    "    precision = c['TP']/(c['TP']+c['FP']) if (c['TP']+c['FP'])>0 else 0.0\n",
    "    recall    = c['TP']/(c['TP']+c['FN']) if (c['TP']+c['FN'])>0 else 0.0\n",
    "    f1        = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0\n",
    "    acc       = (c['TP']+c['TN'])/max(1,(c['TP']+c['TN']+c['FP']+c['FN']))\n",
    "    return {'precision':precision, 'recall':recall, 'f1':f1, 'accuracy':acc}\n",
    "\n",
    "def evaluate_by_detector(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for det, sub in df_results.groupby('detector'):\n",
    "        c = compute_confusion(sub)\n",
    "        m = prf_from_counts(c)\n",
    "        out.append({'detector':det, **c, **m})\n",
    "    return pd.DataFrame(out).sort_values('f1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf44db9",
   "metadata": {},
   "source": [
    "### Cargar resultados y evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = '/content/results.csv'  # reemplaza por tu archivo\n",
    "try:\n",
    "    df_res = pd.read_csv(RESULTS_PATH)\n",
    "    display(df_res.head())\n",
    "    report = evaluate_by_detector(df_res)\n",
    "    display(report)\n",
    "except FileNotFoundError:\n",
    "    print(\"Sube tu archivo a /content como 'results.csv' (usa la plantilla generada).\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
